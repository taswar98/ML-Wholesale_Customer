{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EECS4404 Project 2 - Wholesale Customer\n",
    "#### Taswar Karim \n",
    "#### Student ID: 215095078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#for data visulization in EDA step\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#for data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#for implementing RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#for implementing K-means model\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#for implementing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#for XGBOOST\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#for Ignoring warnings in the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#other settings\n",
    "sns.set()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data to pandas dataframe\n",
    "df=pd.read_csv('Wholesale customers data .csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loaded to pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 440 rows and 8 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking datatypes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns are numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Description of data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the statistical description of each columns, which includes the mean, standerd deviation, min, max and qauntile values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking missing data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing datapoint in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Channel' Column with countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Channel', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Region' Column with countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Region', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Fresh' Column with Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,6))\n",
    "sns.histplot(data=df, x=\"Fresh\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Milk' Column with Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,6))\n",
    "sns.histplot(data=df, x=\"Milk\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Grocery' Column with Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,6))\n",
    "sns.histplot(data=df, x=\"Grocery\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Frozen' Column with Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,6))\n",
    "sns.histplot(data=df, x=\"Frozen\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Detergents_Paper' Column with Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,6))\n",
    "sns.histplot(data=df, x=\"Detergents_Paper\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring 'Delicassen' Column with Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,6))\n",
    "sns.histplot(data=df, x=\"Delicassen\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot=True, fmt='.1g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows the correlation of each column with each other. The closer the value is to 1 it means there is strong positive correlation, while the more its near to -1 it means it is strong negative correlation. Closer to 0 means no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying MinMaxScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler() \n",
    "df_minmax=minmax.fit_transform(df)\n",
    "df_minmax=pd.DataFrame(columns=list(df.columns),data=df_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = StandardScaler()\n",
    "df_standard=standard.fit_transform(df)\n",
    "df_standard=pd.DataFrame(columns=list(df.columns),data=df_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing both scaled methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2,figsize=(20,6))\n",
    "\n",
    "sns.histplot(  x=\"Fresh\", data=df_minmax , ax=axes[0], bins=30)\n",
    "sns.histplot(  x=\"Fresh\", data=df_standard , ax=axes[1], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noticed the distribution for both MinMax and Standard scaler is same. Choosing either one will not make much difference. However, I have chosen to go with MinMax Scaler as it scales in a range of 0-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating Dependent and Independent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_minmax.drop(['Channel'],axis=1)\n",
    "Y = df_minmax['Channel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Number of Feature using RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting classififier\n",
    "svc = SVC(kernel=\"linear\")\n",
    "\n",
    "#Applying RFECV\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(\n",
    "    estimator=svc,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(2),\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    ")\n",
    "\n",
    "#Fitting the data\n",
    "rfecv.fit(X, Y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "plt.plot(\n",
    "    range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),\n",
    "    rfecv.grid_scores_,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of features according to the RFECV method is 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans Clustering for K=2 to K=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "for k in range(2, 15):\n",
    "    kmeanModel = KMeans(n_clusters=k, random_state=0).fit(df_minmax)\n",
    "    cost.append([k,kmeanModel.inertia_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.set_context('poster')\n",
    "plt.plot(pd.DataFrame(cost)[0], pd.DataFrame(cost)[1])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('The Elbow Method showing the optimal k') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noticed that elbow could be taken at k=4. This means we can select k=4 as the optimum number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA with Components=2 and components=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting 2 PCA components\n",
    "pca_2 = PCA(n_components=2)\n",
    "PCA_2 = pca_2.fit_transform(df_minmax)\n",
    "\n",
    "#Selecting 4  components\n",
    "pca_4 = PCA(n_components=4)\n",
    "PCA_4 = pca_4.fit_transform(df_minmax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2 Component Variance:',round(pca_2.explained_variance_ratio_.sum(),5))\n",
    "print('4 Component Variance:',round(pca_4.explained_variance_ratio_.sum(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noticed that 0.89 variance is presented by components = 2 and 0.97 is presented by components = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the 2 components extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "principalDf = pd.DataFrame(data = PCA_2, columns = ['principal component 1', 'principal component 2'])\n",
    "sns.scatterplot(data=principalDf, x=\"principal component 2\", y=\"principal component 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying XGBoost model\n",
    "model = xgboost.XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5) #5 K fold set for here\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% with Standard Deviation of %.2f%%\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy for XGBoost is 90.23 which is very good. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e6c12acef52fb63f5c69123f384ddf637b6e58d8f615616b026921a4cb16581"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
